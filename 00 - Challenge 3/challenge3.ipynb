{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('HeartDiseasePrediction')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Carga de Datos\n",
    "# Cargamos el archivo CSV con los datos del vino y revisamos las primeras filas.\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "\n",
    "gt = spark.read.csv('data/heart_disease_uci.csv', \n",
    "                       inferSchema = True,\n",
    "                       header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocesamiento: Imputar valores nulos\n",
    "# Calcular las medias para las columnas con valores nulos y rellenarlos\n",
    "mean_trestbps = spark_df.select(\"trestbps\").agg({\"trestbps\": \"mean\"}).first()[0]\n",
    "mean_chol = spark_df.select(\"chol\").agg({\"chol\": \"mean\"}).first()[0]\n",
    "mean_thalch = spark_df.select(\"thalch\").agg({\"thalch\": \"mean\"}).first()[0]\n",
    "mean_oldpeak = spark_df.select(\"oldpeak\").agg({\"oldpeak\": \"mean\"}).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar imputación\n",
    "spark_df = spark_df.na.fill({\n",
    "    \"trestbps\": mean_trestbps,\n",
    "    \"chol\": mean_chol,\n",
    "    \"thalch\": mean_thalch,\n",
    "    \"oldpeak\": mean_oldpeak\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convertir columnas booleanas a tipo string\n",
    "# Aquí hacemos la conversión de \"exang\" y cualquier otra columna booleana\n",
    "spark_df = spark_df.withColumn(\"exang\", col(\"exang\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "\n",
    "# 5. Indexar columnas categóricas\n",
    "categorical_cols = [\"sex\", \"cp\", \"restecg\", \"exang\", \"slope\", \"thal\"]\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\").fit(spark_df) for col in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar los indexers al DataFrame\n",
    "for indexer in indexers:\n",
    "    spark_df = indexer.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Continuar con el ensamblado y procesamiento de características como estaba planeado\n",
    "# Ensamblar características\n",
    "feature_cols = [\"age\", \"trestbps\", \"chol\", \"thalch\", \"oldpeak\"] + [col + \"_index\" for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "spark_df = assembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar el tipo de la columna \"features\"\n",
    "print(\"Tipo de 'features' después de ensamblar:\", spark_df.schema[\"features\"].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay valores nulos en las columnas de características\n",
    "spark_df.select([col for col in feature_cols if spark_df.filter(spark_df[col].isNull()).count() > 0]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en las columnas de características\n",
    "spark_df = spark_df.na.drop(subset=feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar algunos valores de la columna \"features\" para ver su contenido\n",
    "spark_df.select(\"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar estadísticas descriptivas para las columnas de características\n",
    "spark_df.select(feature_cols).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar las características\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "spark_df = scaler.fit(spark_df).transform(spark_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProcBigdata25B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
